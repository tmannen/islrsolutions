#Applied

4
---

```
library(e1071)
set.seed(1)
x = matrix(rnorm(100*2), ncol=2)
y=c(rep(-1,60), rep(1, 40))
x[1:30,] = x[1:30,] - 2
x[30:60,] = x[30:60,] + 2
plot(x, col=(3-y))
dat = data.frame(x=x,y=as.factor(y))

train = sample(50, 100)
linear.svm = svm(y~., data=dat[train,], kernel="linear", cost=1)
radial.svm = svm(y~., data=dat[train,], kernel="radial", gamma=1, cost=1)
poly.svm = svm(y~., data=dat[train,], kernel="polynomial", degree=3)

table(true=dat[-train,"y"], pred=predict(linear.svm, newx=dat[-train,]))
table(true=dat[-train,"y"], pred=predict(poly.svm, newx=dat[-train,]))
table(true=dat[-train,"y"], pred=predict(radial.svm, newx=dat[-train,]))
```

Oddly enough the linear works best in the test set, although we can find models with better parameters by crossvalidation. We only tested one values each here. In the training set the radial is easily the best.

5
---

####a)

```
set.seed(1)
x1=runif(500)-0.5
x2=runif(500)-0.5
y=1*(x1^2-x2^2 > 0)
```

####b)

```
plot(x1, x2, col=y+1)
```

####c)

```
datas = data.frame(x1=x1, x2=x2, y=y)
train = sample(nrow(datas), nrow(datas)/2)
log.reg = glm(y~., family=binomial, data=datas, subset=train)
```

####d)

```
yhat <- predict(log.reg, data=datas, subset=train, type="response")
predicted.classes = 1*(yhat>0.5)
plot(x1[train], x2[train], col=predicted.classes+1)
```

From the plot we can see the boundary is linear, but it misses a large part of the classifications.

####e)

```
log.reg.squared = glm(y~I(x1^2) + I(x2^2), data=datas, subset=train)
```

####f)

```
yhat <- predict(log.reg, data=datas, subset=train, type="response")
predicted.classes = 1*(yhat>0.5)
plot(x1[train], x2[train], col=predicted.classes+1)
```

This is kind of cheating, since I used the same formula as when making the data.

####g)

```
svmdata = data.frame(x1=x1, x2=x2, y=as.factor(y))
linear.svm = svm(y~., data=svmdata[train,], kernel="linear", cost=1)
yhat=predict(linear.svm, newx=svmdata[train,])
plot(x1[train], x2[train], col=as.numeric(yhat))
```

####h)

```
poly.svm = svm(y~., data=svmdata[train,], kernel="polynomial", degree=2)
yhat=predict(poly.svm, newx=svmdata[train,])
plot(x1[train], x2[train], col=as.numeric(yhat))
```

####i)

Similar results.